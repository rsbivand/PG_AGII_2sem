# Working with spatial data

```{r pl_io0, echo=FALSE, results="hide"}
io_dir <- "Input_output"
if (!dir.exists(io_dir)) io_dir <- paste0("../", io_dir)
```

```{r echo=FALSE}
load(file = paste0(io_dir, "/talk2.RData"))
```


## Merging spatial data {#sec-sd-merge}

As was mentioned above in section @sec-sd-basics, spatial vector geometries are most often furnished with identification keys, permitting other data using the same key or index identifiers to be added correctly. In Poland, they have been known as `"TERYT"`, and are fairly stable over time, though border changes can occur, with new key values appearing and old values ceasing to be used, leading to breaks of series. The `rgugik` package contains a data object listing these keys together with unit names, but matching on names can be troublesome. First we will subset this object to the Pomeranian voivodeship:

```{r pl_m0, size="footnotesize"}
library(rgugik)
library(sf)
substring(commune_names$TERYT, 1, 2) -> vpom
commune_names |> subset(subset = vpom == "22") -> pom0
```

then retrieve the boundaries of municipalities in Pomerania using `"TERYT"` rather than using the names of municipalities, which are duplicated in other voivodeships:

```{r pl_m1, size="footnotesize", eval=FALSE}
borders_get(TERYT = pom0$TERYT) -> pom
```
```{r pl_m1a, size="footnotesize", echo=FALSE, warning=FALSE}
pom <- try(base::readRDS("Datasets/sd/pom_sf.rds"), silent=TRUE)
if (inherits(pom, "try-error")) pom <- base::readRDS("../Datasets/sd/pom_sf.rds")
```

Merging the `pom` object containing the `"TERYT"` keys and municipality names, with `pom_sf`, including the `"TERYT"` keys and municipality boundaries is easy, as the keys are both character objects - this matters because some keys may start with `0`, zero, and be wrongly read as integers, dropping leading zeros. The key for this voivodeship is `"22"`, so this problem is not encountered here, but often occurs. Should an identifying key with leading zeros be read as integer, it can be restored using `base::formatC` after checking the required string width:

```{r pl_m1b, size="footnotesize"}
ID <- as.integer("0012345") 
str(ID)
formatC(ID, format="d", width=7, flag="0")
```

Note that the `"TERYT"` key values returned in the `pom_sf` object include an underscore between the six digit territorial code and the trailing digit expressing the type of municipality. 

```{r pl_m2, size="footnotesize"}
dim(pom)
pom |> sapply(class) |> str()
str(pom$TERYT)
str(pom0)
```

The underscore needs to be removed before finally checking that they match:

```{r pl_m2a, size="footnotesize"}
pom$TERYT <- sub("_", "", pom$TERYT)
any(is.na(match(pom$TERYT, pom0$TERYT)))
```

Finally, the merging operation may be carried out:

```{r pl_m3, size="footnotesize"}
pom |> merge(pom0, by = "TERYT") -> pom1 
pom1
```

### Using files from Poland's Local Data Bank (BDL)

Having methodically made a first merge, we can move forward with three files at the municipality level from Statistics Poland's [local data bank](https://bdl.stat.gov.pl/bdl/start). These were exported as comma (or semicolon) separated value (CSV) files on February 15, 2024 - dates matter, as online data banks update their contents as inaccuracies are detected. Currently, no package provides direct access to this data bank, and for moderate data volumes, registration is required. The first file is semicolon separated, and contains population data by age group and sex (category K3, group G7, subgroup P2577) for the end of the second half-year 2022:

```{r pl_m4, size="footnotesize", eval=FALSE}
pop22 <- read.csv("Datasets/sd/LUDN_2577_2022.csv", sep=";")
```
```{r pl_m4a, size="footnotesize", echo=FALSE, warning=FALSE}
pop22 <- try(read.csv2("Datasets/sd/LUDN_2577_2022.csv", sep=";"), silent=TRUE)
if (inherits(pop22, "try-error")) pop22 <- read.csv2("../Datasets/sd/LUDN_2577_2022.csv", sep=";")
```

As with most such data, the column names need adjustment. On reading, reserved characters in R are replaced by dots, and for further work, shorter column names are helpful. Current R versions support multibyte characters across all platforms, but some files with single-byte characters can be encountered, in which case judicious use of `base::iconv` may be needed. 

```{r pl_m5, size="footnotesize"}
sapply(pop22, class)
pop22$Kod <- as.character(pop22$Kod)
names(pop22)[3:8] <- c("m_u15", "f_u15",
 "m_15_64", "f_15_59", "m_o65", "k_o60")
pop22$type <- factor(substring(pop22$Kod, 7, 7),
 levels = c("1", "2", "3"),
 labels = c("urban", "rural", "urban_rural"))
names(pop22)
```

The key column was read as integer, so needs correcting; otherwise the remaining columns seem to be formatted acceptably. A new column is created as a factor showing the type of municipality, as a factor  In addition, we The `"X"` column contains no data, and is dropped from the merging operation. We could overwrite the cumulating object on merge; here we choose to create a new object at each merge. The key is called `"TERYT"` in `pom_sf` and `"Kod"` in `pop92`, specified with `by.x` and `by.y` arguments:

```{r pl_m6, size="footnotesize"}
pom1 |> merge(pop22[, -9], by.x = "TERYT",
 by.y = "Kod") -> pom2
pom2
```

Having added the population group counts by sex for the Pomeranian municipalities, we create a new column `"pop"` summing the total population at year end 2022. We then calculate municipality areas, and divide population by area (in square kilometres) to get the population density:

```{r pl_m7, size="footnotesize"}
pom2 |> st_drop_geometry() |> subset(select = 4:9) |>
 apply(1, function(x) sum(x)) -> pom2$pop
pom2 |> st_area() |>
 units::set_units("km2") -> pom2$area_km2
pom2 |> st_drop_geometry() |>
 subset(select = c(pop, area_km2)) |>
 apply(1, function(x) x[1]/x[2]) -> pom2$density
 pom2
```

The next file contains counts of farms from the Agricultural census reporting farm income from the farm (category K34, group G637, subgroup P4139). While this file was exported from the local data bank in the same way as the previous one, it is comma-separated:

```{r pl_m8a, size="footnotesize", eval=FALSE}
ag20 <- read.csv("Datasets/sd/POWS_4139_2020.csv", sep=",")
```
```{r pl_m8b, size="footnotesize", echo=FALSE, warning=FALSE}
ag20 <- try(read.csv("Datasets/sd/POWS_4139_2020.csv", sep=","), silent=TRUE)
if (inherits(ag20, "try-error")) ag20 <- read.csv("../Datasets/sd/POWS_4139_2020.csv", sep=",")
```

Again, column names require simplification, but here there is no spurious `"X"` column - we just drop the municipality name from the merge:

```{r pl_m9, size="footnotesize"}
ag20$Kod <- as.character(ag20$Kod)
names(ag20)[3:7] <- c("farm", "non_farm_business",
 "non_farm_wages", "pension", "other_non_wage")
pom2 |> merge(ag20[,-2], by.x = "TERYT",
 by.y = "Kod") -> pom3
```


Finally, and because the agricultural census data are counts of farms, we need counts of inhabited dwellings from the population census 2021 (category K31, group G645, subgroup P4383), especially for urban municipalities; the merging process follows the lines of those above:

```{r pl_m10a, size="footnotesize", eval=FALSE}
cen21 <- read.csv("Datasets/sd/NARO_4383_2021.csv", sep=";")
```
```{r pl_m10b, size="footnotesize", echo=FALSE, warning=FALSE}
cen21 <- try(read.csv2("Datasets/sd/NARO_4383_2021.csv", sep=";"), silent=TRUE)
if (inherits(cen21, "try-error")) cen21 <- read.csv2("../Datasets/sd/NARO_4383_2021.csv", sep=";")
```
```{r pl_m10c, size="footnotesize"}
cen21$Kod <- as.character(cen21$Kod)
names(cen21)[3] <- "dwellings"
pom3 |> merge(cen21[, -c(2, 4)], by.x = "TERYT",
 by.y = "Kod") -> pom4
names(pom4)
```

In conclusion, for completeness, let us assign aggregation levels to the newly merged values:

```{r pl_m11, size="footnotesize"}
agrs <- factor(c(rep("identity", 3), rep("aggregate", 6),
 "identity", rep("aggregate", 9)),
 levels=c("constant", "aggregate", "identity"))
names(agrs) <- names(st_drop_geometry(pom4))
pom4 |> st_set_agr(agrs) -> pom5
```

### Using API from Poland's Local Data Bank (BDL)

The `bdl` package offers a limited Application Programming Interface (API) to Poland's Local Data Bank (BDL), limited chiefly by the very tight limits on queries per time interval. Details can be found in the [BDL API manual](https://api.stat.gov.pl/Home/BdlApi?lang=en). This means that the user should think through carefully the queries required, and should be familiar with the [categories](https://bdl.stat.gov.pl/bdl/metadane/kategorie) and observational units (local government units and time in years) used. These give very fine-grained access to the underlying data, very similar in structure to that of other providers of official statistics. It is also recommended that a [private API key](https://statisticspoland.github.io/R_Package_to_API_BDL/articles/bdl.html) be acquired and used (here kept in a local environment variable to prevent its display online breaking the privacy requirement of the data provider):

```{r bdl1, size="footnotesize"}
library(bdl)
BDL_key <- Sys.getenv("BDL_key_RSB")
options(bdl.api_private_key=BDL_key)
```

In order to proceed, we need first to determine the level code for voivodeship units, querying BDL for defined levels, and storing level values for voivodeships and municipalities:

```{r bdl2, size="footnotesize"}
(levs <- get_levels(lang="en"))
levs |> as.data.frame() |>
 subset(name=="Poziom Województw", select=id, drop=TRUE) -> wlev 
levs |> as.data.frame() |>
 subset(name=="Poziom Gmin", select=id, drop=TRUE) -> glev 
```

Given the voivodeship level, we can find the 12-character identifier for the Pomeranian voivodeship; BDL uses 12 characters rather tha 7 used in TERYT above, simply interjecting identifiers for other level units (TERYT is characters 3:4 and 8:12):

```{r bdl3, size="footnotesize"}
(units_lev <- get_units(level=wlev, lang="en"))
units_lev |> as.data.frame() |>
 subset(name=="POMORSKIE", select=id, drop=TRUE) -> pom_id
```

Let us see which municipalities belonged to Pomerania in 2020:

```{r bdl4, size="footnotesize"}
pom_gminy <- get_units(parentId=pom_id, level=glev, year=2020)
nrow(pom_gminy)
```

It turns out that some municipalities had seen changes, including changes in categorisation into urban, rural and urban-rural, and some reported here are parts classed as 4 or 5, rather than 1, 2 or 3. If we see a warning indicating that something may not be as we assume, we need more detailed information, returning objects for each municipality ID. The returned object is a list showing the years for which the ID is valid, and the category that it belonged to in those years. We create logical variables expressing whether the municipality ID is valid for our period of interest (2020-2022), and whether the category is 1, 2 or 3:

```{r bdl5, size="footnotesize"}
pom_gminy$id |> sapply(unit_info) -> gminy_details
str(gminy_details[11:12], vec.len=2)
fyears <- function(x) all(2020:2022 %in% x$years)
gminy_details |> sapply(fyears) -> now
table(now)
fkind <- function(x) as.integer(x$kind) < 4L
gminy_details |> sapply(fkind) -> kind123 
table(kind123)
now & kind123 -> nk
table(nk)
gminy_details[nk] -> pom_gminy0
fcols <- function(x) {
  data.frame(x[c("id", "name", "kind")])
}
pom_gminy0 |> lapply(fcols) |>
 do.call("rbind", args = _ ) -> pom_gminy
str(pom_gminy, vec.len=2)
```

Finally we use the logical variables to subset the specification of the municipalities, retaining ID, name and its category as urban, rural or urban-rural.

The capital letters `K`, `G` and `P` are hierarchical, expressing the high-level categories of data topics, medium-level groups of data topics within each category, and sub-groups of data topics within each group. We can list the categories in BDL:

```{r bdl6, size="footnotesize"}
get_subjects(lang="en")
```

Since we are interested in `K3` population, among other things, we can list its contents:

```{r bdl7, size="footnotesize"}
get_subjects("K3", lang="en")
```

and go deeper to `G7` to see that to access population counts in pre-working, working, and post-working age groups are given in `P2577`:

```{r bdl8, size="footnotesize"}
get_subjects("G7", lang="en")
```

From sub-group, we next extract the table of variables, then saving the variable IDs to use for the query; we keep the descriptive table because it gives us the name of each variable in the table:

```{r bdl9, size="footnotesize"}
(get_variables("P2577", lang="en") -> pop_persons)
(as.character(pop_persons$id) -> pop_pre_post_ids)
```

Iterating over the vector of variable IDs, we extract the single variables for multiple units for the required year - omitting the year reports all years. The `get_data_by_variable` call is one query, so using it rather than `get_data_by_unit` iterating over units is advantageous since the query count per unit time is policed vigourously, but instead of reporting only for `r nrow(pom_gminy)` units, the call includes superfluous units:

```{r bdl10, size="footnotesize"}
f1 <- function(i, year=NULL) {
  get_data_by_variable(unitParentId=pom_id,
   level=glev, varId = i, year=year)
}
pop_pre_post_ids |> lapply(f1, year=2021) |> setNames(pop_pre_post_ids) -> res_pop0
res_pop <- vector(mode="list", length=length(pop_pre_post_ids))
str(res_pop0[1], vec.len=2)
f2 <- function(x) {
  x$id %in% pom_gminy$id -> chosen
  subset(x, subset=chosen)
}
res_pop0 |> lapply(f2) -> res_pop
```

The `attrId` values can indicate the need to replace the value reported with `NA`, as some 0 values are set as such because of confidentiality constraints; the attributes have these interpretations:

```{r bdl11, size="footnotesize"}
get_attributes(lang="en")
```

In the case of the population data, all the values are valid as read. `res_pop` is a list of objects, one object per queried variable, so we extract the `attrId` vectors from each object and combine them with `unlist`, all taking the value `1`, counts measured in persons. The zero value if present would be a measured zero count:

```{r bdl12, size="footnotesize"}
res_pop |> lapply("[", "attrId") |> unlist() |> table()
```

Next we need to disambiguate the object variable names `val` using the variable IDs, select only the unit ID and value columns, merge the multiple objects in the list, and drop repeated copies of the unit IDs to yield a data frame with a unit ID column and population count columns:

```{r bdl13, size="footnotesize"}
res_pop[[1]] |> subset(select="id") |> data.frame() -> res_pop1
f_val <- function(x) {
   subset(x=x, select="val", drop=TRUE) 
}
res_pop |> sapply(f_val) |> data.frame() -> res_pop2
res_pop1 |> cbind(res_pop2) -> res_pop3
```

Moving on to the sources of income for farm households from the 2020 Agricultural Census, we can drop the `year` argument, as observation time is given by the data source. First the top-level category contains these variable groups:

```{r bdl14, size="footnotesize"}
get_subjects("K34", lang="en")
```

and group `G637` contains these sub-groups:

```{r bdl15, size="footnotesize"}
get_subjects("G637", lang="en")
```

pointing to `P4139`:

```{r bdl16, size="footnotesize"}
(get_variables("P4139", lang="en") -> agrinc)
as.character(agrinc$id) -> agrinc_ids
```

Repeating the step from annual population counts, we can retrieve the recorded values by variable ID, and drop the observation unit IDs not included in our valid set:

```{r bdl17, size="footnotesize"}
agrinc_ids |> lapply(f1) |> setNames(agrinc_ids) -> res_agr0
res_agr0 |> lapply(f2) -> res_agr
str(res_agr[1], vec.len=2)
```

In this case, however, we see that the `attrId` `91` is present, probably indicating that the household count was too small to be revealed without the risk of confidentiality being breached:

```{r bdl18, size="footnotesize"}
res_agr |> lapply("[", "attrId") |> unlist() |> table()
```

The `val` columns contain zero for these cases, which need to be updated to reflect the fact that the data are missing:

```{r bdl19, size="footnotesize"}
f91 <- function(x) {
  subset(x=x, subset=attrId > 1, select="val")
}
res_agr |> sapply(f91)
```

This can be done using `is.na` for the logical condition that `attrId > 1`, or another choice if other `attrId` values are present that can be accepted, for example preliminary data:

```{r bdl20, size="footnotesize"}
f_NA <- function(x) {
  is.na(x$val) <- x$attrId > 1
  x
}
res_agr |> lapply(f_NA) -> res_agr_NA
res_agr_NA |> sapply(f91)
```

From here we proceed as before to generate an output object with obervation unit rows, and observation unit ID and variable ID columns:

```{r bdl21, size="footnotesize"}
res_agr_NA[[1]] |> subset(select="id") |> data.frame() -> res_agr1
res_agr_NA |> sapply(f_val) |> data.frame() -> res_agr2
res_agr1 |> cbind(res_agr2) -> res_agr3
str(res_agr3, vec.len=2)
```

Finally, the variable from the 2021 Population Census giving the number of dwellings per observation unit is needed to give a basis for the agricultural incomes by household data set; first we look at category `K31`:

```{r bdl22, size="footnotesize"}
get_subjects("K31", lang="en")[16:21,]
```

From this, we see group `G645`:

```{r bdl23, size="footnotesize"}
get_subjects("G645", lang="en")
```

and choose sub-group `P4383`:

```{r bdl24, size="footnotesize"}
(get_variables("P4383", lang="en") -> dwell_hh)
as.character(dwell_hh$id) -> dwell_hh_ids
```

downloading as before and dropping the observation unit IDs identified as invalid for our purposes (wrong year, not urban, rural or urban-rural municipalities):

```{r bdl25, size="footnotesize"}
dwell_hh_ids |> lapply(f1) |> setNames(dwell_hh_ids) -> res_hh0
res_hh0 |> lapply(f2) -> res_hh
str(res_hh[1], vec.len=2)
```

There are no missing data observations:

```{r bdl26, size="footnotesize"}
res_hh |> lapply("[", "attrId") |> unlist() |> table()
```

so we go on as before without the need to insert missing values:

```{r bdl27, size="footnotesize"}
res_hh[[1]] |> subset(select="id") |> data.frame() -> res_hh1
res_hh |> sapply(f_val) |> data.frame() -> res_hh2
res_hh1 |> cbind(res_hh2) -> res_hh3
```

To complete, it would be useful to collate the tables showing the descriptions of the variables, noting that we need to handle multiple description columns:

```{r bdl27a, size="footnotesize"}
list(pop_persons, agrinc, dwell_hh) -> tabs
(tabs |> data.table::rbindlist(fill=TRUE) -> var_desc)
```

Having downloaded the data we have chosen, we merge the sub-group queries into a single object, adding a a `TERYT` column:

```{r bdl28, size="footnotesize"}
res_pop3 |> merge(res_agr3, by="id") |>
 merge(res_hh3, by="id") -> data_df
data_df |> subset(select="id", drop=TRUE) |> substr(3, 4) -> Tw
data_df |> subset(select="id", drop=TRUE) |> substr(8, 12) -> Tg
paste(Tw, Tg, sep="") -> data_df$TERYT
str(data_df, vec.len=2)
```

We complete by merging the municipality boundaries with the imported data using `TERYT` as the ID key:

```{r bdl29, size="footnotesize"}
pom1 |> merge(data_df, by="TERYT") -> pom4_bdl
str(pom4_bdl, vec.len=2)
```


## Visualising spatial data {#sec-sd-vis}

For many purposes, maps themselves are spatial data. Observations are located in space as points,  lines or polygons, and their placing yields useful contextual information when plotted. Using the `plot` methods on `terra` and `sf`, we can display the elevation raster as a background, and show the municipality boundaries with their ID keys:

```{r chile_vis1a, size="footnotesize"}
#| label: base_cl
#| fig-cap: Elevation of Región del Maule, municipalities (base graphics plot methods)
library(elevatr)
library(terra)
maule_sf |>  get_elev_raster(z = 7,
 clip = "locations", neg_to_na = TRUE) |> 
 rast() -> maule_elev
plot(maule_elev, col=rev(hcl.colors(50, "Dark Mint")))
plot(st_geometry(maule_sf), add=TRUE)
text(st_coordinates(st_centroid(st_geometry(maule_sf))),
 label=maule_sf$codigo_comuna)
```

`mapsf` provides fairly flexible map creation functionality for overlaying layers of map information on a graphics output device, following the same order as above, but permitting the improvement of label positioning. Above, we set the palette to be used for elevation to `"Dark Mint"`, which is the default for `mapsf::mf_raster`:

```{r chile_vis1b, size="footnotesize"}
#| label: mapsf_cl
#| fig-cap: Elevation of Región del Maule, municipalities (base graphics mapsf functions)
library(mapsf)
mf_raster(maule_elev, leg_pos="bottomleft",
 leg_title="elevation (m)", leg_horiz=TRUE,
 leg_size=0.8, leg_val_rnd=0)
mf_map(st_geometry(maule_sf), add=TRUE, lwd=2,
 border="grey60", col="transparent")
mf_label(maule_sf, var="codigo_comuna", overlap=FALSE,
 halo=2)
```

`tmap` is about to be upgraded, and is very concise in expressing map construction by adding successive layers to the output graphic with the `+` operator:

```{r chile_vis2a, size="footnotesize"}
#| label: tmap3_cl
#| fig-cap: Elevation of Región del Maule, municipalities (trellis graphics tmap functions)
library(tmap)
tm_shape(maule_elev) + tm_raster(n=15,
 palette=rev(hcl.colors(7, "Dark Mint"))) +
 tm_shape(maule_sf) + tm_borders() +
 tm_text("codigo_comuna")
```

Using the simplest approach, we can plot the positions of the take-away outlets registered for Talca city in OpenStreetMap, and see that they are very clustered within the boundary of the municipality:

```{r chile_vis2, size="footnotesize"}
#| label: takeaways_talca1
#| fig-cap: Location of take-away outlets in Talca
plot(st_geometry(talca_sf))
plot(st_geometry(talca_takeaways), pch=4, col=3,
 cex=2, lwd=2, add=TRUE)
```

Since it would be very useful to obtain more locational context, we can use `mapview` to view the take-away outlets on a web map background, so that we can interact with the point locations. The `mapview` methods convert the object to be displayed to `"OGC:WGS84"` if necessary, then to Web Mercator for display:

```{r chile_vis3a, size="footnotesize", eval=!knitr::is_latex_output()}
#| label: takeaways_talca2
#| fig-cap: Interactive map of take-away outlets in Talca
library(mapview)
mapview(talca_takeaways)
```
```{r chile_vis3b, echo=FALSE, results="as.is", eval=knitr::is_latex_output()}
#| label: takeaways_talca2a
#| fig-cap: Screen dump of interactive map of take-away outlets in Talca
img <- try(knitr::include_graphics("Images/sd/talca_takeaways.png"), silent=TRUE)
if (inherits(img, "try-error")) knitr::include_graphics("../Images/sd/talca_takeaways.png")
```

Moving to the data set for municipalities in the Polish voivodeship of Pomerania, we can attempt to show the density variable on an interactive map:

```{r pl_vis0a, size="footnotesize", eval=!knitr::is_latex_output()}
#| label: gd_dens0a
#| fig-cap: Interactive map of Pomeramia municipalities from rgugik, population density per square kilometre
library(mapview)
mapview(pom5, zcol="density")
```
```{r pl_vis0b, echo=FALSE, results="as.is", eval=knitr::is_latex_output(), warning=FALSE}
#| label: gd_dens0b
#| fig-cap: Screen dump of interactive map of Pomeramia municipalities from rgugik, population density per square kilometre
img <- try(knitr::include_graphics("Images/sd/gd_dens0.png"), silent=TRUE)
if (inherits(img, "try-error")) knitr::include_graphics("../Images/sd/gd_dens0.png")
```

As may be seen in the interactive map, the municipality boundaries do match the Baltic sea coastline quite well, but the jurisdiction of the municipalities extends to some coastal waters in the Gulf of Gdańsk and Zalew Wiślany; we did after all download administrative municipality boundaries using `rgugik`. We need to find another source of boundaries that agree with the coastline, and then need to take the intersection of the two sets od polygons, leaving only areas present in both. `giscoR` provides access to boundaries for European NUTS regions, in geographical coordinates by default. We then need to transform to the Transverse Mercator projection used for the download from `rgugik` before carrying out the intersection:


```{r pl_vis0c, size="footnotesize"}
library(giscoR)
pom_gisco <- gisco_get_nuts(year="2021", resolution="01",
 spatialtype="RG", nuts_id="PL63")
pom_gisco_tm <- st_transform(pom_gisco, "EPSG:2180")
pom6 <- st_intersection(pom5, pom_gisco_tm)
```

On occasion, topological operations on two objects like `st_intersection` fail because the two geometries have different coordinate reference systems, in which case, `st_transform` should be used first to bring them into agreement. If in any topological operation or predicate (such as a query like `st_intersects`), it is found that a geometry is invalid, for example the self-intersection of a polygon boundary, use `st_make_valid` on the invalid object. It can be the case that the geometry cannot be repaired, as some data providers do not check the provided geometries for validity. Since an intersection operation can yield points and lines as well as polygons, we can check the input and output geometries and re-establish their original geometry types; in this case only polygons and multipolygons were output:

```{r pl_vis0c1, size="footnotesize"}
pom5 |> st_geometry_type() |> table() -> tab5; tab5[tab5>0]
pom6 |> st_geometry_type() |> table() -> tab6; tab6[tab6>0]
pom6 |> st_cast("MULTIPOLYGON") -> pom6
```


Since the areas of some municipalities have now changed, we need to re-caluclate both areas and densities:

```{r pl_vis0d, size="footnotesize"}
pom6 |> st_area() |>
 units::set_units("km2") -> pom6$area_km2
pom6 |> st_drop_geometry() |>
 subset(select = c(pop, area_km2)) |>
 apply(1, function(x) x[1]/x[2]) -> pom6$density
```

As can be seen from the figure below, the graphical representation is now more legible; coastlines and boundaries are typically read as contextual location information.

```{r pl_vis0e, size="footnotesize", eval=!knitr::is_latex_output()}
#| label: gd_dens1a
#| fig-cap: Interactive map of Pomeramia municipalities from rgugik, corrected population density per square kilometre
mapview(pom6, zcol="density")
```
```{r pl_vis0f, echo=FALSE, results="as.is", eval=knitr::is_latex_output(), warning=FALSE}
#| label: gd_dens1b
#| fig-cap: Screen dump of interactive map of Pomeramia municipalities from rgugik, corrected population density per square kilometre
img <- try(knitr::include_graphics("Images/sd/gd_dens1.png"), silent=TRUE)
if (inherits(img, "try-error")) knitr::include_graphics("../Images/sd/gd_dens1.png")
```

Two minor points before we move on, first that the distribution of the variable which we want to map does matter. The population density of Pomeranian municipalities is far from being symmetric, as this density plot shows:

```{r pl_vis1a, size="footnotesize"}
#| label: pl_vis1a
#| fig-cap: Density plot of population density, Census 2021, Pomeranian municipalities
plot(density(pom6$density))
```

In this case, in thematic cartography we should create class intervals for display using quantiles or other suitable methods. `mapsf` provides a geometric progression `"geom"` method for creating class intervals for skewed variables:

```{r pl_vis1b, size="footnotesize"}
#| label: pl_vis1b
#| fig-cap: Population density, Census 2021, Pomeranian municipalities
mf_map(pom6, var="density", type="choro", breaks="geom",
 nbreaks=7)
```

The choice of breaks style does matter for communicating the important traits of the variable in question in thematic cartography, here as a choropleth map.

The second display problem is associated with showing the kind of classes implied by the data. For a categorical variable and few classes, the classes are taken from the categories, and unlike the examples above using sequential palettes, this uses a qualitative palette:

```{r pl_vis1c, size="footnotesize"}
#| label: pl_vis1c
#| fig-cap: Municipality type, Pomeranian municipalities
mf_map(pom6, var="type", type="typo")
```

When the variable is crosses a specified mid-point, like regression residuals, a sequential palette is not appropriate, and a diverging palette should be chosen. `tmap::tm_fill` attempts to make reasonable choices in this situation; here the deviance of the proportion of the municipality population less than 16 years old from its mean:

```{r pl_vis2a, size="footnotesize"}
pom6$young <- 100*(pom6$m_u15 + pom6$f_u15)/pom6$pop
pom6$young_res <- residuals(lm(young ~ 1, data=pom6))
```

```{r pl_vis2b, size="footnotesize"}
#| label: pl_vis2b
#| fig-cap: Proportion of population under 16 years of age, residuals from the mean, Pomeranian municipalities, 2022
tm_shape(pom6) + tm_fill("young_res", style="quantile",
 n=11, midpoint=0, palette="RdBu") + tm_borders()
```

```{r pl_vis3, size="footnotesize"}
#| label: pl_vis3
#| fig-cap: Interactive map of take-away outlets in Gdańsk
library(mapview)
mapview(gd_takeaways)
```

## Reading and writing spatial data {#sec-sd-io}


For the present, we will only consider the reading and writing of spatial vector data using functions from `sf`: `st_read` and `st_write`. Further, while it is still regretably the case that many organisations still use the  `"ESRI Shapefile"` (actually most often a bundle of at least four files) for sharing data, the GeoPackage format, `"GPKG"`, should be preferred because it represents CRS properly, does not restrict field (variable) names to 10 characters, does use multibyte characters, and does store numerical fields in binary rather than text format. For commonly used formats, the format-specifiic driver is chosen from the file extension:

```{r pl_io1, size="footnotesize", warning=FALSE}
fn <- paste0(io_dir, "/pom6.gpkg")
if (file.exists(fn)) tull <- file.remove(fn)
try(st_write(pom6, fn))
```

```{r pl_io1a, size="footnotesize", warning=FALSE, echo=FALSE}
lf <- list.files(io_dir, pattern="pom6.gpkg")
if (length(lf) > 0L) tull <-file.remove(paste(io_dir, lf, sep="/"))
```

Unfortunately, our intersection of the GISCO voivodeship outline added a number of variables to the data set, including one called `"FID"` , which is a reserved word for many spatial data formats, meaning "Feature IDentifier", and must be an integer. Now it is a character vector, so we replace it by a compliant integer vector with unique values; we could also have dropped it:

```{r pl_io2, size="footnotesize", warning=FALSE}
str(pom6$FID)
pom6$FID <- 1:nrow(pom6)
st_write(pom6, dsn=fn)
```

To read, `st_read` will try to identify the format and use the appropriate driver automatically; if the data source containd multiple layers, the first will be read by default:

```{r pl_io3a, size="footnotesize"}
pom6a <- st_read(dsn=fn)
```

When we do a simple check to see if output and input are identical - and find that they are not:

```{r pl_io3b, size="footnotesize"}
isTRUE(all.equal(st_geometry(pom6), st_geometry(pom6a)))
```

When written to the external format, the order of the columns changes, as the geometry column is moved to the end on reading. This re-orders the columns, which contain the same information as before but the order differs:

```{r pl_io3c, size="footnotesize"}
names(pom6)
names(pom6a)
```

In addition, the `"area_km2"` column loses its units definition, and the `"type"` column becomes a character vector, not a factor. Such differences are trivial and to be expected, but imply that if an object is to shared with collaborators, any shared script should use the object as read from file, especially if column order rather than column names are used in analysis. After reading, we can restore the representation of the columns before starting work:

```{r pl_io3d, size="footnotesize"}
pom6a$type <- factor(pom6a$type)
pom6a$area_km2 <- units::set_units(pom6a$area_km2, "km2")
```


```{r echo=FALSE}
save(list = ls(), file = paste0(io_dir, "/talk3.RData"))
```

